# Chart-Consistent Mixture-of-Experts Policies for Sample-Efficient Continuous Control
This repository contains a clean PyTorch implementation of the **Chart-Consistent Mixture-of-Experts PPO (CCMoE-PPO)** and the baselines used in our HalfCheetah-v5 experiments:

- **CCMoE-PPO (proposed)** â€“ chart-consistent mixture-of-experts policy with:
  - Shared encoder features
  - Euclidean chart cover in feature space
  - Masked, locality-aware gating
  - Overlap-conditioned consistency on chart intersections
  - Optional gradient balancing for the overlap loss
- **Single-Gaussian PPO** â€“ standard diagonal Gaussian actorâ€“critic baseline
- **MoE-PPO** â€“ mixture-of-experts PPO with global gate (no chart structure)
- **Graph-Laplacian PPO** â€“ hard Voronoi partition + parameter-space Laplacian penalty

> âš ï¸ **Important**: To fully understand the design choices, losses, and hyperparameters in this code, you **should read the corresponding CCMoE paper**. The paper explains the geometry, consistency constraints, and the theoretical motivation that this implementation follows.

---

## 1. File Overview

The main script (you can rename it as you wish, e.g. `ccmoe_halfcheetah.py`) implements:

- **Algorithms**
  - `SingleGaussianPPO` â€“ vanilla PPO
  - `MoEPPO` â€“ mixture-of-experts PPO
  - `GraphLaplacianPPO` â€“ PPO + graph Laplacian regularization on expert heads
  - `CCMoE` â€“ proposed chart-consistent mixture-of-experts policy

- **Key components**
  - `MLP` â€“ shared encoder \( \phi_\theta(o) \)
  - `ChartCover` â€“ maintains the chart centers and radii in feature space
  - `Gate` â€“ gating network producing expert logits
  - `LocalGaussianHead` â€“ per-chart Gaussian policies
  - `SquashedDiagGaussian` â€“ tanh-squashed diagonal Gaussian policy
  - `RolloutBuffer` â€“ storage for PPO rollouts with GAE(Î»)
  - `RunLogger` â€“ CSV + JSON logging for each run

- **Experiment orchestration**
  - `run_one(...)` â€“ run one training configuration
  - `run_suite(...)` â€“ run full grid of CCMoE + baselines
  - `aggregate_summary()` â€“ aggregate all summaries into `aggregate_summary.json` and `.csv`
  - Command-line interface for configuring runs

All experiment outputs are stored under:

```text
CCMoE_HalfCheetah/
  â”œâ”€â”€ ccmoe/
  â”œâ”€â”€ ppo/
  â”œâ”€â”€ ppo_glap/
  â”œâ”€â”€ moe/
  â”œâ”€â”€ master_index.csv
  â”œâ”€â”€ aggregate_summary.csv
  â””â”€â”€ aggregate_summary.json
```
- Each algorithm subfolder contains one directory per run with:
  - `config.json` â€“ serialized `TrainConfig`
  - `summary.json` â€“ final metrics (AUC, final return, time-to-threshold, etc.)
  - `episode.csv` â€“ per-episode returns vs. environment steps
  - `train.csv` â€“ PPO + regularizer diagnostics
  - `checkpoints/` â€“ model checkpoints
The main script (e.g. ccmoe_halfcheetah.py) implements:

- Algorithms:
  - `CCMoE` â€“ proposed chart-consistent mixture-of-experts policy
  - `SingleGaussianPPO` â€“ standard PPO
  - `MoEPPO` â€“ mixture-of-experts PPO baseline
  - `GraphLaplacianPPO` â€“ PPO + graph Laplacian baseline
- Core components:
  - `MLP` â€“ shared encoder
  - `ChartCover` â€“ maintains chart centers and radius in feature space
  - `Gate` â€“ gating network producing expert logits
  - `LocalGaussianHead` â€“ per-chart Gaussian policies
  - `SquashedDiagGaussian` â€“ tanh-squashed diagonal Gaussian
  - `RolloutBuffer` â€“ PPO rollouts with GAE(Î»)
  - `RunLogger` â€“ CSV + JSON logging
- Experiment orchestration:
  - `run_one(...)` â€“ run one training configuration
  - `run_suite(...)` â€“ run CCMoE + baselines over a hyperparameter grid
  - `aggregate_summary(...)` â€“ aggregate results across runs
## 2. Running Experiments
 - **Run the full suite (CCMoE + baselines + grid over hyperparameters)**
   This reproduces the grid described in the paper for HalfCheetah-v5 (you may adjust the ranges to exactly match the paper):
```text
python ccmoe_halfcheetah.py \
  --run_all \
  --steps 800000 \
  --seeds 0,1,2 \
  --m_list 2,4,8 \
  --r_list 1.5,2.0,2.5 \
  --lambda_list 0.0,0.01,0.05 \
  --restrictions identity,learned
```
What this does:
 -  Runs CCMoE-PPO over a grid of:
    - Number of charts `m`
    - Chart radius `r`
    - Overlap penalty weight `Î»`
    - Restriction type (`identity` or `learned`)
 - Runs PPO, MoE-PPO, and Graph-Laplacian PPO baselines with compatible settings.
 - Logs the results under CCMoE_HalfCheetah/ and aggregates metrics.
You can run just CCMoE or any single baseline:
**CCMoE-PPO (proposed)**
```text
python ccmoe_halfcheetah.py \
  --algo ccmoe \
  --seeds 0 \
  --steps 800000 \
  --m_list 4 \
  --r_list 2.0 \
  --lambda_list 0.01 \
  --restrictions identity \
  --gate_ent_coef 0.0 \
  --grad_balance_alpha 0.0
```
**Single-Gaussian PPO**
```text
python ccmoe_halfcheetah.py \
  --algo ppo \
  --seeds 0 \
  --steps 800000
```
**MoE-PPO baseline**
```text
python ccmoe_halfcheetah.py \
  --algo moe \
  --m_list 4 \
  --seeds 0 \
  --steps 800000
```
**Graph-Laplacian PPO baseline**
```text
python ccmoe_halfcheetah.py \
  --algo ppo_glap \
  --m_list 4 \
  --r_list 2.0 \
  --k_lap 2 \
  --lap_scale 1e-4 \
  --seeds 0 \
  --steps 800000
```
> ðŸ’¡ **Note**: For single-run mode (`--run_all off`), the script reads only the first value of each `*_list` argument (e.g. the first element of `--m_list`, `--r_list`, `--lambda_list`).

## 3. Important Hyperparameters (see the paper for details)
The following options are exposed via TrainConfig and the CLI.
The paper is the source of truth for how they should be set to reproduce the reported results.
**Algorithm**
  - `--algo` in `{ccmoe, ppo, ppo_glap, moe}`
**Charts / Cover**
  - `--m_list` â€“ number of charts `m`
  - `--r_list` â€“ chart radius `r` in whitened feature space
  - `--restrictions` â€“ `"identity"` or `"learned"` restriction maps
**CCMoE Overlap / Regularization**
  - `--lambda_list` â€“ maximum overlap penalty `Î»_max` (or fixed `Î»` if gradient balancing is off)
  - `--lam_min` â€“ lower bound `Î»_min` during gradient balancing
  - `--grad_balance_alpha` â€“ gradient balancing coefficient `Î±`
  - `--gate_ent_coef` â€“ gate entropy coefficient `Î·`
**PPO Core**
  - `--steps` â€“ total environment steps per run
  - `--update_freq` â€“ steps per PPO update (rollout batch size)
  - `--epochs` â€“ PPO epochs per update
  - `--mb_size` â€“ mini-batch size within PPO update
  - `--lr` â€“ learning rate
  - `--vf_coef`, `--ent_coef` â€“ value loss and policy entropy weights
  - `--clip_ratio` â€“ PPO clipping range
**Graph-Laplacian baseline**
  - `--k_lap` â€“ `k` for k-NN graph on chart centers
  - `--lap_scale` â€“ Laplacian penalty weight
To understand why these hyperparameters exist and how they relate to CCMoEâ€™s manifold and overlap structure, please read the paper. The paper explains the theoretical motivation and the recommended grids.

 ## 4. Reproducing the Paper
To reproduce the experimental results:
 1. Read the CCMoE paper carefully. It explains:
    - The geometry of the chart cover and restrictions
    - The exact form of the consistency loss and gradient balancing
    - The hyperparameter grids and training protocol
    - Implementation details that are not obvious from code alone
 2. Match the hyperparameters:
    - Set `--m_list, --r_list, --lambda_list, --gate_ent_coef, --grad_balance_alpha, etc.`, according to the experimental section of the paper.
 3. Run multiple seeds:
    - Use the same number of seeds as in the paper (e.g. `--seeds 0,1,2` or more) and aggregate the results.
 4. Post-process logs. Use:
    - episode.csv
    - train.csv
    - aggregate_summary.json
    to build your evaluation tables and plots.
 > **Again**: reading the paper is necessary if you want to fully understand the process and reproduce the reported results.

 ## 5. Citing
 **If you use this code in your research, please cite the CCMoE paper** 
```text
@software{messlem_2025,
	author       = {Messlem, Abdelkader and
	Messlem, Youcef},
	title        = {Chart-Consistent Mixture-of-Experts PPO (CCMoE-
	PPO)
	},
	month        = nov,
	year         = 2025,
	publisher    = {Zenodo},
	version      = {1.0.0},
	doi          = {10.5281/zenodo.17605495},
	url          = {https://doi.org/10.5281/zenodo.17605495},
}
```
